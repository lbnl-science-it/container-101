<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="June 10, 2020" />
  <title>Container 101 on Lawrencium</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Container 101 on Lawrencium</h1>
<p class="author">June 10, 2020</p>
<p class="date">Wei Feinstein</p>
</header>
<h1 id="outline">Outline</h1>
<ul>
<li>Lawrencium Supercluster overview</li>
<li>Computing resources &amp; services<br />
</li>
<li>Container technology overview</li>
<li>Build singularity containers</li>
<li>Run singularity containers on Lawrencium</li>
</ul>
<h1 id="lawrencium-cluster-overview">Lawrencium Cluster Overview</h1>
<p><left><img src="figures/lrc.png" width="70%"></left></p>
<p><a href="https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/lbnl-supercluster/lawrencium">Detailed informaton of Lawrencium</a></p>
<h1 id="accounts-on-lawrencium">Accounts on Lawrencium</h1>
<h3 id="three-types-of-project-accounts">Three types of Project Accounts</h3>
<ul>
<li>PI Computing Allowance (PCA) account: free 300K SUs per year (pc_xxx)</li>
<li>Condo account: PIs buy in compute nodes to the general condo pool (lr_xxx)</li>
<li>Recharge account: with minimal recharge rate ~ $0.01/SU (ac_xxx)</li>
</ul>
<p><a href="%5Bhttps://sites.google.com/a/lbl.gov/hpc/getting-an-account">https://sites.google.com/a/lbl.gov/hpc/getting-an-account</a></p>
<h3 id="user-accounts">User accounts</h3>
<ul>
<li>User account request</li>
<li>User agreement consent</li>
</ul>
<p><a href="https://sites.google.com/a/lbl.gov/hpc/getting-an-account">https://sites.google.com/a/lbl.gov/hpc/getting-an-account</a></p>
<h1 id="softwre-module-farm">Softwre Module Farm</h1>
<h3 id="module-commands">Module commands</h3>
<ul>
<li><em>module purge</em>: clear user’s work environment</li>
<li><em>module avail</em>: check available software packages</li>
<li><em>module load xxx</em>: load a package</li>
<li><em>module list</em>: check currently loaded software</li>
<li>Users may install their own software</li>
</ul>
<p><a href="https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/sl6-module-farm-guide">https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/sl6-module-farm-guide</a></p>
<h1 id="slurm-resource-manager-job-scheduler">SLURM: Resource Manager &amp; Job Scheduler</h1>
<h3 id="jub-submission">Jub Submission</h3>
<ul>
<li>Get help with the complete command options</li>
</ul>
<pre><code>sbatch --help</code></pre>
<ul>
<li>sbatch: submit a job to the batch queue system</li>
</ul>
<pre><code>sbatch myjob.sh</code></pre>
<ul>
<li>srun: request an interactive node(s) and login automatically</li>
</ul>
<pre><code>srun -A ac_xxx -p lr5 -q lr_normal -t 1:0:0 --pty bash</code></pre>
<ul>
<li>salloc : request an interactive node(s)</li>
</ul>
<pre><code>salloc –A pc_xxx –p lr6 –q lr_debug –t 0:30:0</code></pre>
<h1 id="job-monitoring">Job Monitoring</h1>
<ul>
<li>sinfo: check status of partitions and nodes (idle, allocated, drain, down)</li>
</ul>
<pre><code>sinfo –r –p lr6</code></pre>
<ul>
<li>squeue: check jobs in the batch queuing system (R or PD)</li>
</ul>
<pre><code>squeue –u $USER</code></pre>
<ul>
<li>sacct: check job information or history</li>
</ul>
<pre><code>sacct -X -o &#39;jobid,user,partition,nodelist,stat&#39;</code></pre>
<ul>
<li>scancel : cancel a job</li>
</ul>
<pre><code>scancel jobID</code></pre>
<ul>
<li>Check slurm association, such as qos, account, partition</li>
</ul>
<pre><code>sacctmgr show association user=wfeinstein -p
perceus-00|ac_test|wfeinstein|lr6|1||||||||||||lr_debug,lr_lowprio,lr_normal|||
perceus-00|ac_test|wfeinstein|lr5|1||||||||||||lr_debug,lr_lowprio,lr_normal|||
perceus-00|pc_test|wfeinstein|lr4|1||||||||||||lr_debug,lr_lowprio,lr_normal|||
perceus-00|lr_test|wfeinstein|lr3|1||||||||||||lr_debug,lr_lowprio,lr_normal,te
perceus-00|scs|wfeinstein|es1|1||||||||||||es_debug,es_lowprio,es_normal|||</code></pre>
<p><a href="https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/scheduler/slurm-usage-instructions">https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/scheduler/slurm-usage-instructions</a></p>
<h1 id="services-1">Services (1)</h1>
<ul>
<li>Designate data transfer node <strong>lrc-xfer.lbl.gov</strong>
<ul>
<li>scp -r /your/source/file $USER@lrc-xder.lbl.gov:/cluster/path</li>
<li>rsync -avzh /your/source/file $USER <span class="citation" data-cites="lrc-xfer.lbl.gov:/cluster/path">@lrc-xfer.lbl.gov:/cluster/path</span></li>
</ul></li>
<li>Globus Online provide secured unified interface for data transfer
<ul>
<li>endpoint lbn#lrc, Globus Connect, AWS S3 connector</li>
</ul></li>
<li>Visualization and remote desktop node <strong>viz.lbl.gov</strong>
<ul>
<li>Detailed information <a href="https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/remote-desktop">click here</a></li>
</ul></li>
</ul>
<h1 id="service-2-jupyterhub">Service (2): Jupyterhub</h1>
<p><a href="https://lrc-jupyter.lbl.gov">LRC Jupyterhub</a> <left><img src="figures/jupyter.png" width="60%"></left></p>
<h1 id="services-3-cloud-computing">Services (3): Cloud Computing</h1>
<ul>
<li>LBNL has a master payer program for cloud services on Amazon Web Services (AWS) and Google Cloud Platform (GCP).<br />
</li>
<li>No charge to have an account in the program</li>
<li>Charges only for actual usage of cloud services like storage or compute</li>
<li>Monthly billing for cloud usage are direct to your PID via recharge mechanism</li>
<li>Simple enrollment process.</li>
<li>De-enrollment only changes the billing setup in your account, and your account will continue to be active.</li>
<li>Complete control of your own AWS or GCP dashboard, your data, tools, and services.</li>
</ul>
<h1 id="aws-gcp-services">AWS &amp; GCP Services</h1>
<ul>
<li>AWS and GCP make discounts available to LBNL users.</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Type</th>
<th>AWS</th>
<th>GCP</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Overall</td>
<td>7%</td>
<td>13%</td>
<td></td>
</tr>
<tr class="even">
<td>Data Egress</td>
<td>15%</td>
<td>25%</td>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li>AWS, <a href="https://aws.amazon.com/blogs/publicsector/aws-offers-data-egress-discount-to-researchers/">restrictions apply</a></li>
<li>Over 125 people at LBNL with cloud accounts on AWS and GCP.<br />
</li>
<li>Mostly use virtual machines and storage, containers, ML, AI, and data visualization</li>
<li>To set up a cloud account on either AWS or GCP, send email to <a href="mailto:scienceit@lbl.gov">scienceit@lbl.gov</a></li>
</ul>
<h1 id="virtual-machine-services">Virtual Machine Services</h1>
<p><left><img src="figures/vm.png" width="40%"></left></p>
<p>More information of VM click <a href="https://commons.lbl.gov/display/itfaq/SVM+-+Virtual+Machine+Hosting">here</a></p>
<h1 id="containerization">Containerization</h1>
<ul>
<li>Standardized packaging of software and dependencies</li>
<li>Portable, shareable, and reproducible.</li>
<li>Your application brings its environment with it.</li>
<li>Share the same OS kernel</li>
</ul>
<h4 id="applications">Applications</h4>
<ul>
<li>Package an analysis pipeline so that it runs on your laptop, in the cloud, and in HPC environment to produce the same result.</li>
<li>Publish a paper and include a link to a container with all of the data and software that you used so that others can easily reproduce your results.</li>
<li>Install and run an application that requires a complicated stack of dependencies</li>
<li>Legacy codes require outdated OS</li>
</ul>
<h1 id="container-vs.-virtual-machine">Container vs. Virtual Machine</h1>
<p><left><img src="figures/vm-sif.png" width="45%"></left></p>
<h1 id="singularity-technology">Singularity Technology</h1>
<ul>
<li>Open-source computer software that encapsulates an application and all its dependencies into a single image</li>
<li>Bring containers and reproducibility to scientific computing and HPC</li>
<li>Developed by Greg Kurtzer</li>
<li>Typically users have a build system as root users, but may not be root users on a production system</li>
</ul>
<h1 id="docker">Docker</h1>
<ul>
<li>Bring containerization to the community-scale</li>
<li>Rich image repository</li>
<li>Widely used by scientific communities</li>
<li>Compose for defining multi-container, recipe/definition file to build docker images</li>
<li>Security concerns not ideal for the HPC environment</li>
</ul>
<p>Learn more <a href="https://docs.docker.com/get-docker/">docker</a></p>
<h1 id="singularity-workflow">Singularity Workflow</h1>
<ul>
<li>Install Singularity on a local machine</li>
<li>Build Singularity images locally with a root permission</li>
<li>Transfer images to LRC clusters</li>
<li>Run containers on the cluster
<ul>
<li>Root privilege is not permitted</li>
</ul></li>
</ul>
<h1 id="singularity-installation">Singularity Installation</h1>
<p>OS platforms</p>
<pre><code>- Linux
- Mac
- Window</code></pre>
<p>Refer to instructions <a href="https://github.com/lbnl-science-it/container-101/blob/master/singularity_installation_guide.md">here</a> for details</p>
<p>Test your installation:</p>
<pre><code>$ singularity --version
  singularity version 3.2.1-1 
$ singularity run docker://godlovedc/lolcow
 ______________________________________
/ A tall, dark stranger will have more \
\ fun than you.                        /
 --------------------------------------
        \   ^__^
         \  (oo)\_______
            (__)\       )\/\
                ||----w |
                ||     ||</code></pre>
<h1 id="create-singularity-containers">Create Singularity Containers</h1>
<ul>
<li>Build directly from pre-built docker images
<ul>
<li><a href="https://hub.docker.com/search?q=&amp;type=image">Docker hub</a></li>
<li><a href="https://cloud.sylabs.io/library">Sylabs Cloud</a> and <a href="https://singularity-hub.org/">Singularity hub</a></li>
</ul></li>
<li>More involved using docker images from other external resources
<ul>
<li><a href="https://www.nvidia.com/en-us/gpu-cloud/containers/">Nvidia HPC containers</a></li>
<li><a href="https://biocontainers.pro/#/registry">Biocontainers</a></li>
<li><a href="https://aws.amazon.com/releasenotes/available-deep-learning-containers-images/">AWS</a></li>
</ul></li>
<li>Build using definition files or recipes</li>
</ul>
<h1 id="singularity-pull-directly-from-docker-shub">Singularity pull directly from docker:// &amp; shub://</h1>
<ul>
<li>No root/sudo privilege is needed</li>
<li>Create/download immutable squashfs images/containers</li>
</ul>
<pre><code>singularity pull --help</code></pre>
<ul>
<li>Docker Hub:  Pull a container from Docker Hub.</li>
</ul>
<pre><code>$ singularity pull docker://ubuntu:18.04 
$ singularity pull docker://gcc:7.2.0</code></pre>
<ul>
<li>Singularity Hub:  If no tag is specified, the master branch of the repository is used</li>
</ul>
<pre><code>$ singularity pull hello-world.sif shub://singularityhub/hello-world</code></pre>
<h1 id="run-singularity-with-shell-run-exec">Run Singularity with shell, run, exec</h1>
<ul>
<li><strong>shell</strong> sub-command: invokes an interactive shell within a container</li>
</ul>
<pre><code>singularity shell hello-world.sif</code></pre>
<ul>
<li><strong>run</strong> sub-command: executes the container’s runscript</li>
</ul>
<pre><code>singularity run hello-world.sif</code></pre>
<ul>
<li><strong>exec</strong> sub-command: execute an arbitrary command within container</li>
</ul>
<pre><code>singularity exec hello-world.sif cat /etc/os-release</code></pre>
<h1 id="singularity-pull-when-docker-containers-provided-by-other-external-docker-repositories">Singularity pull when Docker Containers Provided by Other External docker repositories</h1>
<ul>
<li><a href="https://www.nvidia.com/en-us/gpu-cloud/containers/">Nvidia HPC containers</a></li>
<li><a href="https://biocontainers.pro/#/registry">Biocontainers</a></li>
<li><a href="https://aws.amazon.com/releasenotes/available-deep-learning-containers-images/">AWS</a></li>
</ul>
<h2 id="steps-to-build-singularity-containers-from-ngc">Steps to build singularity containers from NGC:</h2>
<ul>
<li>Pull a docker image locally (e.g. pgi compiler from NGC)</li>
</ul>
<pre><code>$ docker pull nvcr.io/hpc/pgi-compilers:ce
.....

$ docker images
REPOSITORY                     TAG              IMAGE ID            CREATED             SIZE
nvcr.io/hpc/pgi-compiler        ce            c13ce6cf7f66        6 months ago        9.9GB
openmpi3.1                    latest          08a5518bb344        9 months ago        14.3GB
registry                         2            f32a97de94e1        15 months ago       25.8MB
...</code></pre>
<h1 id="steps-to-build-singularity-containers-from-ngc-1">Steps to build singularity containers from NGC:</h1>
<ul>
<li><p>Push to the Docker Hub (docker login) or simply use your local docker images</p></li>
<li><p>Singularity build from local registry</p></li>
</ul>
<pre><code>$ singularity build pgi.sif docker-daemon://nvcr.io/hpc/pgi-compilers:ce
$ ls
Dokerfile  hello-world.sif  pgi.sif  saxpy.c  Singularity</code></pre>
<ul>
<li>Compile OpenACC code</li>
</ul>
<pre><code>$ singularity exec pgi.sif pgcc -o saxpy -acc -Minfo saxpy.c
saxpy:
      9, Loop is parallelizable
         Generating Tesla code
          9, #pragma acc loop gang, vector(128) /* blockIdx.x threadIdx.x */
      9, FMA (fused multiply-add) instruction(s) generated
...</code></pre>
<ul>
<li>Execute binary ./saxpy</li>
</ul>
<pre><code>$ singularity exec pgi.sif ./saxpy
y[0] = 2.000000</code></pre>
<p>Another example of using docker containers from <a href="https://github.com/lbnl-science-it/singularity_aws_dl_container">AWS</a></p>
<h1 id="singularity-build">Singularity build</h1>
<ul>
<li>Root/sudo privilege is needed</li>
</ul>
<pre><code>singularity build --help</code></pre>
<ul>
<li>Build from a definition file</li>
</ul>
<pre><code>sudo singularity --debug build mycontainer.sif Singularity </code></pre>
<h1 id="definition-filerecipe">Definition File/Recipe</h1>
<pre><code>Bootstrap: docker
#library, docker, shub, localimage, yum, debootstrap, arch, busybox, zypper 
From: ubuntu

# used singularity run-help 
%help
Hello. I&#39;m in the container.

# executed on host after the base OS is installed.
%setup
    touch ${SINGULARITY_ROOTFS}/tacos.txt
    echo &quot;I love avocado&quot; &gt;&gt; avocados.txt

# copy files from your host system into the container 
%files
    avocados.txt /opt    

%environment
  export NAME=avocado

# executed within the container after the base OS is installed at build time
#install new software and libraries, config files,  directories, etc
%post
    echo &#39;export Avocado=TRUE&#39; &gt;&gt; $SINGULARITY_ENVIRONMENT

# executed when the container image is run:  singularity run
%runscript 
    echo &quot;Hello! Arguments received: $* \n&quot;
    exec echo &quot;$@&quot;  </code></pre>
<p>More information of <a href="https://sylabs.io/guides/3.0/user-guide/definition_files.html">singularity recipes</a></p>
<h1 id="singularity-build-rewritable-sandbox">Singularity Build Rewritable Sandbox</h1>
<ul>
<li>Can be built from a recipe or existing container</li>
<li>Used to develop, test, and make changes, then build or convert it into a standard image</li>
</ul>
<pre><code>sudo singularity build --sandbox gccbox docker://gcc:7.2.0
sudo singularity build --sandbox test-box Singularity </code></pre>
<ul>
<li>When you want to alter your image, you can use commands like shell, exec, run, with the –writable option</li>
</ul>
<pre><code>sudo singularity shell --writable test-box</code></pre>
<ul>
<li>Convert a sandbox to an immutable final container:</li>
</ul>
<pre><code>sudo singularity build test-box.sif test-box</code></pre>
<h1 id="inspect-containers">Inspect Containers</h1>
<ul>
<li>To check how a image is built, running script and environment variables..</li>
</ul>
<pre><code>singularity inspect [options] image_name
    --lablels
    --runscript
    --deffile
    --environment
e.g. singularity inspect --deffile mycontainer.sif</code></pre>
<h1 id="singularity-python-spython">Singularity Python (spython)</h1>
<ul>
<li>Python API for Singularity containers</li>
<li>Convert Dockerfile to Singularity def</li>
</ul>
<pre><code>spython recipe Dockerfile &gt; dock2sif.def</code></pre>
<h1 id="run-singularity-containers-on-lawrencium">Run Singularity Containers on Lawrencium</h1>
<ul>
<li>File transfer to LRC cluster</li>
</ul>
<pre><code>scp xxx.sif $USER@lrc-xfer.lbl.gov:/your/path/on/cluster </code></pre>
<ul>
<li>Run your container interactively
<ul>
<li>Request an interactive compute node</li>
</ul>
<pre><code>singularity shell/run/exec container.sif</code></pre></li>
<li>Submit a slurm job</li>
</ul>
<h1 id="job-submission-example">Job Submission Example</h1>
<pre><code>#!/bin/bash -l
#SBATCH --job-name=container-test        
#SBATCH --partition=lr5          
#SBATCH --account=ac_xxx         
#SBATCH --qos=lr_normal         
#SBATCH --nodes=1           
#SBATCH --time=1-2:0:0          

cd $SLURM_SUBMIT_DIR
singularity exec mycontainer.sif cat /etc/os-release</code></pre>
<h1 id="container-bind-path">Container Bind Path</h1>
<ul>
<li>Singularity allow mapping directories on host to directories within container</li>
<li>Easy data access within containers</li>
<li>System-defined bind paths on LRC
<ul>
<li>/global/home/users/</li>
<li>/global/scratch/</li>
</ul></li>
<li>User can define own bind paths:</li>
<li>e.g.: mount /host/path/ on the host to /container/path inside the container</li>
</ul>
<pre><code>-B /host/path/:/container/path

singularity run --nv -B /global/home/users/$USER:/tmp pytorch_19_12_py3.sif ls /tmp</code></pre>
<h1 id="run-gpu-and-mpi-containers">Run GPU and MPI Containers</h1>
<ul>
<li>Singularity supports NVIDIA’s CUDA GPU compute framework or AMD’s ROCm solution</li>
<li>–nv enables NVIDIA GPU support in Singularity</li>
<li>Remember to request a GPU node from the ES1 partition</li>
</ul>
<pre><code>singularity exec --nv pytorch_19_12_py3.sif python -c &quot;import torch; print(torch.__version__)&quot;
1.4.0a0+a5b4d78</code></pre>
<h2 id="run-mpi-containers">Run MPI Containers</h2>
<ul>
<li>If launch on multiple nodes, MPI libraries on the host and inside the container need to match</li>
<li>If launch on one node, only MPI library inside the container is called</li>
</ul>
<h1 id="exercise">Exercise</h1>
<ul>
<li><ol type="1">
<li>singularity pull hello-world.sif shub://singularityhub/hello-world</li>
</ol></li>
<li><ol start="2" type="1">
<li>generate hello-world.def or generate a sandbox to start with</li>
</ol></li>
<li><ol start="3" type="1">
<li>add /data directory inside the container</li>
</ol></li>
<li><ol start="4" type="1">
<li>build a new image hello-world-new.sif</li>
</ol></li>
<li><ol start="5" type="1">
<li>bind /home/$USER on host to /data inside container</li>
</ol></li>
<li><ol start="6" type="1">
<li>ls /data inside the new container</li>
</ol></li>
</ul>
<h1 id="getting-help">Getting help</h1>
<ul>
<li>Virtual Office Hours:
<ul>
<li>Time: 10:30am - noon (Wednesdays)</li>
<li>Request <a href="https://docs.google.com/forms/d/e/1FAIpQLScBbNcr0CbhWs8oyrQ0pKLmLObQMFmYseHtrvyLfOAoIInyVA/viewform">online</a></li>
</ul></li>
<li>Sending us tickets at hpcshelp@lbl.gov</li>
<li>More information, documents, tips of how to use LBNL Supercluster <a href="http://scs.lbl.gov">http://scs.lbl.gov/</a></li>
<li>DLab consulting: <a href="https://dlab.berkeley.edu/consulting">https://dlab.berkeley.edu/consulting</a></li>
</ul>
<p>Please fill out <a href="https://docs.google.com/forms/d/e/1FAIpQLSdrmW-7gZ8FankQwEceY6r_uXPmLHAuXFjDDfwu-86A1a0llg/viewform">Training Survey</a> to get your comments and help us improve.</p>
</body>
</html>
